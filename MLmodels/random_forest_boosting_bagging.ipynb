{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging, Random forests and Boosting\n",
    "\n",
    "The objective of this pratical is to show the use of bagging, random forests, boosting for classification and regression problems, in python with Sciki-learn. This document freely reproduces some examples presented in the Scikit-learn documentation.\n",
    "\n",
    "These statistical training algorithms are based on the idea of combining the predictions of several predictors (or classifiers) for a better generalization and to compensate for possible individual predictors defects. \n",
    "\n",
    "In general, there are two families of methods of this type:\n",
    "\n",
    "    1) Methods by averaging (bagging, random forests) where the principle is to average several predictions hoping a better result following the reduction of the variance of the average estimator.\n",
    "\n",
    "    2) Adaptive methods (boosting) where the parameters are iteratively adapted to produce a better mixture.\n",
    "    \n",
    "In the following we will explore each of these algorithm  in Scikit-learn and present some comparisons.\n",
    "\n",
    "###  Bagging\n",
    "Bagging builds multiple instances of an estimator, calculated on random samples from the training data (and possibly some of the attributes, also randomly selected), and then combine the individual predictions by averaging them to reduce the variance of the estimator.Their main advantage lies in the fact that they build an improved version of the basic decision trees algorithm, without asking for a modification of this algorithm but it is expensive in computation. As they reduce overfitting, bagging methods work very well with \"strong\" predictors.\n",
    "\n",
    "In *Scikit-learn*, bagging methods are implemented via the $BaggingClassifier$ and $BaggingRegressor$ class. The constructors take in parameters an estimator and the strategy of points selection and attributes:\n",
    "\n",
    "* $base\\_estimator$: optional (default = None). If None then the estimator is a decision tree.\n",
    "* $max\\_samples$ : The size of the random sample taken from the training base.\n",
    "* $max\\_features$ : the number of randomly drawn attributes.\n",
    "* $bootstrap$: boolean, optional (default = True). Draw points with a discount or not.\n",
    "* $bootstrap\\_features$: boolean, optional (default = False). Drawing attributes with a discount or not.\n",
    "* $oob\\_score$: boolean. Estimate or not the OOB (Out of Bag) generalization error.\n",
    "\n",
    "The following code builds a set of basic classifiers of type $KNeighborsClassifier$, each using a sample of $50\\%$ training points and $50\\%$ attributes (features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the digits database, which contains 10 classes (handwritten figures images). There are 1797 items, each item has 64 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "# afficher une des images\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "plt.imshow(digits.images[1796])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic classifier is a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "digits = load_digits()\n",
    "X=digits.data\n",
    "y=digits.target\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n",
    "accuracy=clf.score(X,y)\n",
    "\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the basis of training accuracy = 1. For more realism, we cut the base in training / test in order to see the behavior of the tree on data different from those of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7317676143386898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.90)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "Z = clf.predict(X_test)\n",
    "accuracy=clf.score(X_test,y_test)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Construct the variance of the value \"accuracy\" on $200$ draws for the training/test separation. What can we conclude?\n",
    "\n",
    "To compare, we will build a bagging classifier on our data with a $DecisionTreeClassifier$ base classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8763906056860321\n"
     ]
    }
   ],
   "source": [
    "clf = BaggingClassifier(tree.DecisionTreeClassifier(), max_samples=0.2, max_features=0.3, n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "Z = clf.predict(X_test)\n",
    "accuracy = clf.score(X_test,y_test)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Construct the variance of the value \"accuracy\" on $100$ draws for the training/test separation. What can we conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Vary the $max\\_samples$ and $max\\_features$. For which values is the best result obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "The random forest algorithm provides optimization of decision trees. It uses the same principle as bagging, but with an additional randomization step in the selection of node attributes in order to reduce the variance of the obtained estimator. The two Python objects that implement random forests are $RandomForestClassifier$ and $RandomForestRegressor$. The most important parameters are:\n",
    "\n",
    "\n",
    "* $n\\_estimators$: integer, optional (default = 10). The number of trees.\n",
    "* $max\\_features$ : the number of attributes to consider at each split.\n",
    "* $max\\_samples$: The size of the random sample taken from the training base.\n",
    "* $min\\_samples\\_leaf$: The minimum number of elements in a leaf node.\n",
    "* $oob\\_score$: boolean. Estimate or not the OOB (Out of Bag) generalization error.\n",
    "\n",
    "We will redo the classification on the Digits database using a classifier $RandomForestClassifier:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9227441285537701\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "digits = load_digits()\n",
    "X=digits.data\n",
    "y=digits.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.90)\n",
    "clf = RandomForestClassifier(n_estimators=200)\n",
    "clf.fit(X_train, y_train)\n",
    "Z = clf.predict(X_test)\n",
    "accuracy = clf.score(X_test,y_test)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "How does the value of accuracy compare with the bagging case that uses the same number of trees ($200$ in our case)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Construct the variance of the accuracy value on $100$ draws for the training / test. What can we conclude by comparing with the previous section (bagging)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Look in the documentation for  $ExtraTreesClassifier$ and redo the classification with this type of classifier. Compare with $RandomForestClassifier$. And explain the extract contribution with Extra Tress Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "The principle of boosting is to evaluate a sequence of weak classifiers (weak learners) on several slightly modified versions of the training data. The decisions obtained are then combined by a weighted sum to obtain the final model.\n",
    "\n",
    "In Scikit-learn it is the $AdaBoostClassifier$ class that implements the algorithm. The most important parameters are:\n",
    "\n",
    "* $n\\_estimators$: integer, optional (default = 10). The number of weak classifiers.\n",
    "* $learning\\_rate$: controls the rate of change of weights by iteration.\n",
    "* $base\\_estimator$: (default = DecisionTreeClassifier) the weak classifier used.\n",
    "\n",
    "In the following we will redo the classification on the Digits database using a classifier $RandomForestClassifier$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88442521631644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "digits = load_digits()\n",
    "X=digits.data\n",
    "y=digits.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.90)\n",
    "clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=5), n_estimators=200, learning_rate=2)\n",
    "clf.fit(X_train, y_train)\n",
    "Z = clf.predict(X_test)\n",
    "accuracy=clf.score(X_test,y_test)\n",
    "\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "The $max\\_depth$ parameter controls the depth of the tree. Try multiple values to see the impact of using a weaker vs stronger classifier ($max\\_depth$ high or eliminate the parameter). Also, test the effect of the $learning\\_rate$ parameter and the number of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
